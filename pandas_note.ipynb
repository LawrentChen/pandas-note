{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Pandas 使用笔记\n",
        "\n",
        "Python 中处理二维表数据的核心， Python 中的 Excel\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "\n",
        "import numpy as np # 当然少不了和 numpy 搭配使用\n",
        "import pandas as pd"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 纵向合并某一文件夹内所有的 Excel 文件\n",
        "\n经典任务，实现方法有很多，但就是还没有一个统一简洁的官方方法"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def MERGE(workdir):\n",
        "    # 获取路径内全部文件，这里假设文件夹内有且只有目标的excel文件。\n",
        "    # 参杂其他东西也是有办法的，但这里先不涉及那么多\n",
        "    filenames = os.listdir(workdir)\n",
        "    filepath = []\n",
        "    for i in range(0,len(filenames)):\n",
        "        filepath.append(os.path.join(workdir,filenames[i]))\n",
        "        \n",
        "    # 遍历获取所有 excel 内容\n",
        "    content = []\n",
        "    \n",
        "    for file in filepath:\n",
        "        # pandas 读取 excel\n",
        "        # 根据需要可以增加参数:\n",
        "        #   sheet_name 指定工作簿 \n",
        "        #   skipfooter 忽略尾部 \n",
        "        #   dtype 指定列变量类型，详见文档\n",
        "        fileread = pd.read_excel(file)\n",
        "\n",
        "        # 全部加入 content 列表中\n",
        "        content.append(fileread)\n",
        "    \n",
        "    # 将 content 列表中的内容合并为 DataFrame\n",
        "    result = pd.concat(content) # concat 默认为纵向合并\n",
        "    \n",
        "    return result"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 横向合并\n",
        "\n同样有多种方法，可以实现类似 sql 的各种 join"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge方法，还有更多的合并参数可以选择，详见文档\n",
        "df3 = df1.merge(df2, how='outer', on='column_name') # 指定合并根据列\n",
        "df3 = df1.merge(df2, how='outer', left_index=True, right_index=True) # 根据两侧索引合并\n",
        "\n",
        "# Join方法，常用于分割列后的返回合并\n",
        "split = df['column_name'].str.split(expand=True) # 默认按空格分割，且展开列\n",
        "df = df.join(split)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 更多具体操作"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "df = MERGE('filepath')\n",
        "\n",
        "# 重设 pandas 默认索引，常用于合并多个 dataframe 之后\n",
        "df.reset_index(drop=True)\n",
        "\n",
        "# 使用某一列作为索引\n",
        "df.set_index([''], inplace=True)\n",
        "\n",
        "# 输出，可选参数众多，有需要时详见 IO 文档\n",
        "df.to_excel('filename', sheet_name='', index=True, index_label='', merge_cells=False) # 分割合并单元格\n",
        "df.to_csv('filename')\n",
        "\n",
        "# 去除行/列\n",
        "df.drop(columns=['column_name'], inplace=True) # 关键在于 inplace，此参数默认为 false，只会生成一个新的对象\n",
        "df.drop(['column_name'], axis=0, inplace=True) # 效果同上，axis=0 即列\n",
        "\n",
        "# 赋值新建列\n",
        "df['new_column_name'] = 'new_value'\n",
        "df['index1'] = list(range(1, len(allmerge['index'])+1)) # 给一个list，新建一个从1开始的索引\n",
        "\n",
        "# 按某列取值筛选行（类似 excel 的筛选），可组合条件使用\n",
        "df[df[('column_name1']=='value1'&df['column_name2']=='value2')|df['column_name3'=='value3']\n",
        "df = df[df['column_name']!='value'&-df['column_name'].str.contains('str1')&-df['column_name'].str.match('str2')]\n",
        "# 注意上面的负号，可以对布尔值取补集\n",
        "\n",
        "# 给定规则替换值，至少有 map 和 replace 两种方法\n",
        "namemap = {\n",
        "'被替换值':'替换值',\n",
        "'中华人民共和国':'中国',\n",
        "}\n",
        "df['column_name'] = df['column_name'].map(namemap) # map 方法没有 inplace 参数，只能重新赋值\n",
        "# map 方法还可以使用函数，更名副其实。详见文档\n",
        "\n",
        "# replace 方法，这里可以配合正则使用\n",
        "pattern1 = r'.*身份证$'\n",
        "pattern2 = r'.*签证$'\n",
        "# regex 参数用于启动正则模式\n",
        "df['column_name'].replace( [pattern1, pattern2], ['身份证', '签证'], regex=True, inplace=True)\n",
        "\n",
        "# 处理文本两侧可能有的多余空格\n",
        "df['column_name'] = df['column_name'].str.lstrip() # 左去除\n",
        "df['column_name'] = df['column_name'].str.rstrip() # 右去除\n",
        "\n",
        "# 处理 csv 中常见的引号文本标记\n",
        "df['column_name']=df['column_name'].str.strip('\\'')\n",
        "\n",
        "# 列改名\n",
        "df.rename(columns = {'old_name':'new_name'}, inplace=True)\n",
        "\n",
        "# 缺失值 nan 需要用 numpy 处理\n",
        "df.replace(np.nan, 0, inplace = True)\n",
        "\n",
        "# 索引筛选，假设为时间序列数据。\n",
        "# 这一部分方法众多，特别是涉及到多层行/列结构的时候，需要时精读文档\n",
        "df = df.loc['2018-02-08':]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 日期处理\n",
        "\n日期列读入 pandas 后往往只是一般的 object 类型，需要先转换成 datetime 类型方便后续使用"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Datetime 日期格式转换。\n",
        "df['new_datetime'] = pd.to_datetime(df['old_datetime'])\n",
        "# 一次转换多列\n",
        "df[['time1', 'time2']] = df[['time1', 'time2']].apply(pd.to_datetime)\n",
        "\n",
        "# Datetime 日期拆解\n",
        "df['date'] = df['new_datetime'].dt.date\n",
        "df['time'] = df['new_datetime'].dt.time\n",
        "df['secs'] = df['datetime1'] - df['datetime2']\n",
        "df['secs'] = df['secs'].dt.total_seconds()\n",
        "df[['secs1','secs2']] = df[['secs1','secs2']].astype('timedelta64[h]') # 根据需要调整 timedelta 类型"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 数据透视表\n",
        "\n类似 Excel 的效果"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# pivot 数据透视表\n",
        "\n",
        "df_pivot = df.pivot_table(values=['value_column_name'], \n",
        "                          index=['row_column_name'], \n",
        "                          columns=['column_column_name', \n",
        "                                   aggfunc='pd.Series.nunique', \n",
        "                                   margins=True)\n",
        "# 参数：\n",
        "#   Pandas  | Excel\n",
        "#   values  |  值\n",
        "#   index   |  行\n",
        "#   columns |  列\n",
        "#   aggfunc | 汇总方法\n",
        "#  margins-是否显示加总行/列\n",
        "\n",
        "# pivot 表 stack\n",
        "df_pivot = df_pivot.stack().to_frame() # 小心 stack()方法生成的是个 series 对象，还需要进一步转化为 dataframe"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python3"
    },
    "nteract": {
      "version": "0.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}